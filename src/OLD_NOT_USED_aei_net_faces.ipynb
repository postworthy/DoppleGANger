{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ðŸ¤ª AEI_NET - CelebA Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235cbd1-f136-411c-88d9-f69f270c0b96",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own AEI_NET on the CelebA faces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fda92c-d4f3-459b-a60d-e7e75f68dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINING COULD TAKE A WHILE BUT ONLY NEEDS TO BE RUN ONE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47059fe8-bb75-42c4-b930-b9d1a5c6a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_inswapper import do_inswapper_pretraining\n",
    "do_inswapper_pretraining(use_fixed_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# Limit TensorFlow to 80% of GPU memory\n",
    "from gpu_memory import limit_gpu_memory \n",
    "limit_gpu_memory(0.35)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "from notebooks.utils import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras import mixed_precision\n",
    "#mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "NUM_FEATURES = 128\n",
    "Z_DIM = 200\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 1\n",
    "BETA = 2000\n",
    "LOAD_MODEL = True\n",
    "TAKE_BATCHES = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_analysis import FaceAnalysis\n",
    "from inswapper import INSwapper\n",
    "from face import Face\n",
    "\n",
    "PROVIDERS = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "face_analyser = FaceAnalysis()\n",
    "face_analyser.prepare(ctx_id=0, det_size=(640, 640))\n",
    "inswapper = INSwapper('/root/.insightface/models/inswapper_128.onnx')\n",
    "emap = inswapper.emap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73e5a4-1638-411c-8d3c-29f823424458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(file_path):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    #image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "data_dir = \"/app/data/celeba-dataset/img_align_celeba/img_align_celeba/\"\n",
    "file_pattern = f\"{data_dir}*.jpg\"\n",
    "file_count = len([f for f in os.listdir(data_dir) if f.endswith('.jpg')])\n",
    "list_ds = tf.data.Dataset.list_files(file_pattern, shuffle=False)\n",
    "\n",
    "train_data = list_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "train_data = train_data.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc018fea-f881-4ec5-9667-8d901ff45f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(img):\n",
    "    return tf.cast(img, \"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d133a6-5d06-43db-a2e1-841c809e2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fixed image 999999.jpg and preprocess it\n",
    "fixed_img_from_path = \"/app/data/celeba-dataset/img_align_celeba/img_align_celeba/999999.jpg\"\n",
    "fixed_img_from = load_and_preprocess_image(fixed_img_from_path)\n",
    "use_fixed_image = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae2f0d-59fd-4796-841f-7213eae638de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(img_into, img_from):\n",
    "    def process_image(img_into_tensor, img_from_tensor):\n",
    "        try:\n",
    "            # Convert tensor to NumPy array\n",
    "            img_into_np = img_into_tensor.numpy()\n",
    "            img_from_np = img_from_tensor.numpy()\n",
    "\n",
    "            # Get the embedding using OpenCV-based ArcFace model\n",
    "            faces_into = face_analyser.get(img_into_np)\n",
    "            faces_from = face_analyser.get(img_from_np)\n",
    "            faces_into_sorted = sorted(faces_into, key=lambda x: x.bbox[0])\n",
    "            faces_from_sorted = sorted(faces_from, key=lambda x: x.bbox[0])\n",
    "            if faces_into_sorted and faces_from_sorted:\n",
    "                face_into = faces_into_sorted[0]\n",
    "                face_from = faces_from_sorted[0]\n",
    "                result = inswapper.get(img_into_np, face_into, face_from, paste_back=True)\n",
    "                embed = face_from.normed_embedding\n",
    "                embed = np.dot(embed, emap)\n",
    "                embed /= np.linalg.norm(embed)\n",
    "                return result.astype(np.float32), embed\n",
    "            else:\n",
    "                # Generate Gaussian noise with the same shape as img_np\n",
    "                noise = np.random.normal(loc=127.5, scale=50.0, size=img_into_np.shape)\n",
    "                # Clip values to ensure they are within [0, 255]\n",
    "                noise = np.clip(noise, 0, 255).astype(np.uint8)\n",
    "                embed = np.random.normal(size=(512,)).astype(np.float32)\n",
    "                return noise.astype(np.float32), embed\n",
    "        except Exception as e:\n",
    "            print(f\"Error while in process_image:\\n{e}\")\n",
    "            # Generate Gaussian noise with the same shape as img_np\n",
    "            noise = np.random.normal(loc=127.5, scale=50.0, size=img_into_np.shape)\n",
    "            # Clip values to ensure they are within [0, 255]\n",
    "            noise = np.clip(noise, 0, 255).astype(np.uint8)\n",
    "            embed = np.random.normal(size=(512,)).astype(np.float32)\n",
    "            return noise.astype(np.float32), embed\n",
    "            \n",
    "\n",
    "\n",
    "    border_size = 50\n",
    "\n",
    "    img_into_padded = tf.pad(\n",
    "        img_into,\n",
    "        paddings=[[border_size, border_size], [border_size, border_size], [0, 0]],\n",
    "        mode='CONSTANT',\n",
    "        constant_values=255,\n",
    "    )\n",
    "\n",
    "    img_from_padded = tf.pad(\n",
    "        img_from,\n",
    "        paddings=[[border_size, border_size], [border_size, border_size], [0, 0]],\n",
    "        mode='CONSTANT',\n",
    "        constant_values=255,\n",
    "    )\n",
    "\n",
    "    # Wrap the processing function with tf.py_function\n",
    "    Y_target_padded, embed = tf.py_function(func=process_image, inp=[img_into_padded, img_from_padded], Tout=(tf.float32, tf.float32))\n",
    "    Y_target = Y_target_padded[\n",
    "        border_size:-border_size,   \n",
    "        border_size:-border_size,   \n",
    "        :\n",
    "    ]\n",
    "    \n",
    "    return Y_target, embed\n",
    "\n",
    "    \n",
    "#functions.preprocess_face(img=image_path, target_size=(112, 112), enforce_detection=False)\n",
    "\n",
    "# Function to generate random embed and package inputs and outputs\n",
    "def prepare_inputs(img):\n",
    "    # Preprocess the image\n",
    "    img_processed = preprocess(img)\n",
    "    # Get the batch size dynamically\n",
    "    # Generate random embed with shape (batch_size, 256)\n",
    "    #embed = tf.random.normal([batch_size, 256])  # Assuming c_id = 256\n",
    "    # The model expects inputs: [image, embed]\n",
    "    #Y_target = img_processed  # Target output image\n",
    "    # Use tf.py_function to wrap get_target\n",
    "\n",
    "    shuffled_indices = tf.random.shuffle(tf.range(BATCH_SIZE))\n",
    "    img_random = tf.gather(img, shuffled_indices)\n",
    "    indices = tf.range(BATCH_SIZE)\n",
    "    def get_target_pair(idx):\n",
    "        img_i = img[idx]          # Original image\n",
    "        img_j = img_random[idx]   # Randomly selected image\n",
    "        if use_fixed_image:\n",
    "            img_j = fixed_img_from\n",
    "        return get_target(img_i, img_j)\n",
    "    \n",
    "    #Y_target = tf.map_fn(get_target, img, dtype=tf.float32)\n",
    "    Y_target, embed = tf.map_fn(get_target_pair, indices, dtype=(tf.float32, tf.float32))\n",
    "    \n",
    "    Y_target.set_shape(img.shape)\n",
    "    embed.set_shape([BATCH_SIZE, 512])\n",
    "    \n",
    "    Y_target = preprocess(Y_target)\n",
    "\n",
    "    return ((img_processed, embed), Y_target)\n",
    "\n",
    "\n",
    "# Apply the mapping to your dataset\n",
    "train = train_data.map(prepare_inputs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Prefetch data to improve latency\n",
    "train = train.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f32fd-addb-4c9b-906c-a5f1934df7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "((img_batch, embed_batch), Y_target) = next(iter(train))\n",
    "train_sample = img_batch.numpy()\n",
    "result_sample = Y_target.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53709f-7f3f-483b-9db8-2e5f9b9942c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some faces from the training set\n",
    "display(train_sample, 8, cmap=None)\n",
    "display(result_sample, 8, cmap=None)\n",
    "print(result_sample[0])\n",
    "print(embed_batch.shape)\n",
    "print(embed_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build the AEI_NET <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad9761-9756-45b3-83ef-ee3d9218d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aei_net import get_model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## 3. Train the AEI_NET <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429fdad-ea9c-45a2-a556-eb950d793824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the variational autoencoder\n",
    "optimizer = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=['mse', None])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525e44b-b3bb-489c-9d35-fcfe3e714e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_callbacks import get_callbacks\n",
    "model_checkpoint_callback, tensorboard_callback, image_generator = get_callbacks(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080d9a6-8f53-4984-9f80-5e139e6c8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load old weights if required\n",
    "if LOAD_MODEL:\n",
    "    model.load_weights(\"./models/aei_net\")\n",
    "    tmp = model.predict(train.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c497b7-fa40-48df-b2bf-541239cc9400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_elements = file_count\n",
    "total_batches = -(-total_elements // BATCH_SIZE)\n",
    "total_loops = -(-total_batches // TAKE_BATCHES)\n",
    "\n",
    "print(f\"Total batches: {total_batches}\")\n",
    "print(f\"Total epochs needed: {total_loops}\")\n",
    "\n",
    "for i in range(total_loops):\n",
    "    print(f\"{i + 1} of {total_loops}...\")\n",
    "\n",
    "    model.fit(\n",
    "        train.skip(i*TAKE_BATCHES).take(TAKE_BATCHES),\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[\n",
    "            model_checkpoint_callback,\n",
    "            #tensorboard_callback,\n",
    "            image_generator,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Save the final models\n",
    "    model.save(\"./models/aei_net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd76ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682fb369-33fb-4f16-a601-47db56de3fd2",
   "metadata": {},
   "source": [
    "## 3. Reconstruct using the variational autoencoder <a name=\"reconstruct\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1fece5-77a8-4510-be7d-713cc08aee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the test set\n",
    "((img_batch, embed_batch), Y_target) = next(iter(train))\n",
    "example_images = img_batch.numpy()\n",
    "batch_size = tf.shape(img_batch)[0]\n",
    "example_embed = tf.random.normal([batch_size, 256]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7fba06-6a5f-49c2-82a7-e6265acf1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create autoencoder predictions and display\n",
    "Y_pred, z_attr_pred = model.predict([example_images, example_embed])\n",
    "predicted_images = np.clip(Y_pred * 255, 0, 255).astype(np.uint8)\n",
    "print(\"Example real faces\")\n",
    "display(example_images)\n",
    "print(\"Reconstructions\")\n",
    "display(predicted_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f21e8c0-331f-4f73-a73c-8a0e4c1e45dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
